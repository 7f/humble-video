/* ----------------------------------------------------------------------------
 * This file was automatically generated by SWIG (http://www.swig.org).
 * Version 2.0.0
 *
 * Do not make changes to this file unless you know what you are doing--modify
 * the SWIG interface file instead.
 * ----------------------------------------------------------------------------- */

package io.humble.video;
import io.humble.ferry.*;
/**
 * Information about how video data is formatted in an {@link IVideoPicture} 
 * object.  
 * This specifies the color space and how many bits pixel data takes. 
 * It also  
 * includes some utility methods for dealing with {@link Type#YUV420P} 
 * data; the  
 * most common type of encoding used in video files I've run across. 
 *  
 */
public class IPixelFormat extends RefCounted {
  // JNIHelper.swg: Start generated code
  // >>>>>>>>>>>>>>>>>>>>>>>>>>>
  /**
   * This method is only here to use some references and remove
   * a Eclipse compiler warning.
   */
  @SuppressWarnings("unused")
  private void noop()
  {
    IBuffer.make(null, 1);
  }
   
  private volatile long swigCPtr;

  /**
   * Internal Only.
   */
  protected IPixelFormat(long cPtr, boolean cMemoryOwn) {
    super(VideoJNI.SWIGIPixelFormatUpcast(cPtr), cMemoryOwn);
    swigCPtr = cPtr;
  }
  
  /**
   * Internal Only.
   */
  protected IPixelFormat(long cPtr, boolean cMemoryOwn,
      java.util.concurrent.atomic.AtomicLong ref)
  {
    super(VideoJNI.SWIGIPixelFormatUpcast(cPtr),
     cMemoryOwn, ref);
    swigCPtr = cPtr;
  }
    
  /**
   * Internal Only.  Not part of public API.
   *
   * Get the raw value of the native object that obj is proxying for.
   *   
   * @param obj The java proxy object for a native object.
   * @return The raw pointer obj is proxying for.
   */
  public static long getCPtr(IPixelFormat obj) {
    if (obj == null) return 0;
    return obj.getMyCPtr();
  }

  /**
   * Internal Only.  Not part of public API.
   *
   * Get the raw value of the native object that we're proxying for.
   *   
   * @return The raw pointer we're proxying for.
   */  
  public long getMyCPtr() {
    if (swigCPtr == 0) throw new IllegalStateException("underlying native object already deleted");
    return swigCPtr;
  }
  
  /**
   * Create a new IPixelFormat object that is actually referring to the
   * exact same underlying native object.
   *
   * @return the new Java object.
   */
  @Override
  public IPixelFormat copyReference() {
    if (swigCPtr == 0)
      return null;
    else
      return new IPixelFormat(swigCPtr, swigCMemOwn, getJavaRefCount());
  }

  /**
   * Compares two values, returning true if the underlying objects in native code are the same object.
   *
   * That means you can have two different Java objects, but when you do a comparison, you'll find out
   * they are the EXACT same object.
   *
   * @return True if the underlying native object is the same.  False otherwise.
   */
  public boolean equals(Object obj) {
    boolean equal = false;
    if (obj instanceof IPixelFormat)
      equal = (((IPixelFormat)obj).swigCPtr == this.swigCPtr);
    return equal;
  }
  
  /**
   * Get a hashable value for this object.
   *
   * @return the hashable value.
   */
  public int hashCode() {
     return (int)swigCPtr;
  }
  
  // <<<<<<<<<<<<<<<<<<<<<<<<<<<
  // JNIHelper.swg: End generated code
  
  public enum Type {
  /**
   * Pixel format. Notes:
   * RGB32 is handled in an endian-specific manner. A RGBA
   * color is put together as:
   * (A << 24) | (R << 16) | (G << 8) | B
   * This is stored as BGRA on little endian CPU architectures and ARGB 
   * on
   * big endian CPUs.
   * When the pixel format is palettized RGB (PAL8), the palettized
   * image data is stored in AVFrame.data[0]. The palette is transported 
   * in
   * AVFrame.data[1] and, is 1024 bytes long (256 4-byte entries) and 
   * is
   * formatted the same as in RGB32 described above (i.e., it is
   * also endian-specific). Note also that the individual RGB palette 
   *
   * components stored in AVFrame.data[1] should be in the range 0..255. 
   *
   * This is important as many custom PAL8 video codecs that were designed 
   *
   * to run on the IBM VGA graphics adapter use 6-bit palette components. 
   *
   * None
   */
    AV_PIX_FMT_NONE(VideoJNI.IPixelFormat_AV_PIX_FMT_NONE_get()),
  /**
   *
   */
    AV_PIX_FMT_YUV420P,
  /**
   * < packed YUV 4:2:2, 16bpp, Y0 Cb Y1 Cr
   */
    AV_PIX_FMT_YUYV422,
  /**
   * < packed RGB 8:8:8, 24bpp, RGBRGB...
   */
    AV_PIX_FMT_RGB24,
  /**
   * < packed RGB 8:8:8, 24bpp, BGRBGR...
   */
    AV_PIX_FMT_BGR24,
  /**
   *
   */
    AV_PIX_FMT_YUV422P,
  /**
   *
   */
    AV_PIX_FMT_YUV444P,
  /**
   * < planar YUV 4:1:0, 9bpp, (1 Cr & Cb sample per 4x4 Y samples)
   */
    AV_PIX_FMT_YUV410P,
  /**
   *
   */
    AV_PIX_FMT_YUV411P,
  /**
   * < Y , 8bpp
   */
    AV_PIX_FMT_GRAY8,
  /**
   * < Y , 1bpp, 0 is white, 1 is black, in each byte pixels are ordered 
   * from the msb to the lsb
   */
    AV_PIX_FMT_MONOWHITE,
  /**
   * < Y , 1bpp, 0 is black, 1 is white, in each byte pixels are ordered 
   * from the msb to the lsb
   */
    AV_PIX_FMT_MONOBLACK,
  /**
   * < 8 bit with PIX_FMT_RGB32 palette
   */
    AV_PIX_FMT_PAL8,
  /**
   * < planar YUV 4:2:0, 12bpp, full scale (JPEG), deprecated in favor 
   * of PIX_FMT_YUV420P and setting color_range
   */
    AV_PIX_FMT_YUVJ420P,
  /**
   * < planar YUV 4:2:2, 16bpp, full scale (JPEG), deprecated in favor 
   * of PIX_FMT_YUV422P and setting color_range
   */
    AV_PIX_FMT_YUVJ422P,
  /**
   * < planar YUV 4:4:4, 24bpp, full scale (JPEG), deprecated in favor 
   * of PIX_FMT_YUV444P and setting color_range
   */
    AV_PIX_FMT_YUVJ444P,
  /**
   * < XVideo Motion Acceleration via common packet passing
   */
    AV_PIX_FMT_XVMC_MPEG2_MC,
    AV_PIX_FMT_XVMC_MPEG2_IDCT,
  /**
   * < packed YUV 4:2:2, 16bpp, Cb Y0 Cr Y1
   */
    AV_PIX_FMT_UYVY422,
  /**
   * < packed YUV 4:1:1, 12bpp, Cb Y0 Y1 Cr Y2 Y3
   */
    AV_PIX_FMT_UYYVYY411,
  /**
   * < packed RGB 3:3:2, 8bpp, (msb)2B 3G 3R(lsb)
   */
    AV_PIX_FMT_BGR8,
  /**
   * < packed RGB 1:2:1 bitstream, 4bpp, (msb)1B 2G 1R(lsb), a byte contains 
   * two pixels, the first pixel in the byte is the one composed by the 
   * 4 msb bits
   */
    AV_PIX_FMT_BGR4,
  /**
   * < packed RGB 1:2:1, 8bpp, (msb)1B 2G 1R(lsb)
   */
    AV_PIX_FMT_BGR4_BYTE,
  /**
   * < packed RGB 3:3:2, 8bpp, (msb)2R 3G 3B(lsb)
   */
    AV_PIX_FMT_RGB8,
  /**
   * < packed RGB 1:2:1 bitstream, 4bpp, (msb)1R 2G 1B(lsb), a byte contains 
   * two pixels, the first pixel in the byte is the one composed by the 
   * 4 msb bits
   */
    AV_PIX_FMT_RGB4,
  /**
   * < packed RGB 1:2:1, 8bpp, (msb)1R 2G 1B(lsb)
   */
    AV_PIX_FMT_RGB4_BYTE,
  /**
   * < planar YUV 4:2:0, 12bpp, 1 plane for Y and 1 plane for the UV components, 
   * which are interleaved (first byte U and the following byte V)
   */
    AV_PIX_FMT_NV12,
  /**
   * < as above, but U and V bytes are swapped
   */
    AV_PIX_FMT_NV21,
  /**
   * < packed ARGB 8:8:8:8, 32bpp, ARGBARGB...
   */
    AV_PIX_FMT_ARGB,
  /**
   * < packed RGBA 8:8:8:8, 32bpp, RGBARGBA...
   */
    AV_PIX_FMT_RGBA,
  /**
   * < packed ABGR 8:8:8:8, 32bpp, ABGRABGR...
   */
    AV_PIX_FMT_ABGR,
  /**
   * < packed BGRA 8:8:8:8, 32bpp, BGRABGRA...
   */
    AV_PIX_FMT_BGRA,
  /**
   * < Y , 16bpp, big-endian
   */
    AV_PIX_FMT_GRAY16BE,
  /**
   * < Y , 16bpp, little-endian
   */
    AV_PIX_FMT_GRAY16LE,
  /**
   * < planar YUV 4:4:0 (1 Cr & Cb sample per 1x2 Y samples)
   */
    AV_PIX_FMT_YUV440P,
  /**
   * < planar YUV 4:4:0 full scale (JPEG), deprecated in favor of PIX_FMT_YUV440P 
   * and setting color_range
   */
    AV_PIX_FMT_YUVJ440P,
  /**
   * < planar YUV 4:2:0, 20bpp, (1 Cr & Cb sample per 2x2 Y & A samples) 
   *
   */
    AV_PIX_FMT_YUVA420P,
  /**
   * < H.264 HW decoding with VDPAU, data[0] contains a vdpau_render_state 
   * struct which contains the bitstream of the slices as well as various 
   * fields extracted from headers
   */
    AV_PIX_FMT_VDPAU_H264,
  /**
   * < MPEG-1 HW decoding with VDPAU, data[0] contains a vdpau_render_state 
   * struct which contains the bitstream of the slices as well as various 
   * fields extracted from headers
   */
    AV_PIX_FMT_VDPAU_MPEG1,
  /**
   * < MPEG-2 HW decoding with VDPAU, data[0] contains a vdpau_render_state 
   * struct which contains the bitstream of the slices as well as various 
   * fields extracted from headers
   */
    AV_PIX_FMT_VDPAU_MPEG2,
  /**
   * < WMV3 HW decoding with VDPAU, data[0] contains a vdpau_render_state 
   * struct which contains the bitstream of the slices as well as various 
   * fields extracted from headers
   */
    AV_PIX_FMT_VDPAU_WMV3,
  /**
   * < VC-1 HW decoding with VDPAU, data[0] contains a vdpau_render_state 
   * struct which contains the bitstream of the slices as well as various 
   * fields extracted from headers
   */
    AV_PIX_FMT_VDPAU_VC1,
  /**
   * < packed RGB 16:16:16, 48bpp, 16R, 16G, 16B, the 2-byte value for 
   * each R/G/B component is stored as big-endian
   */
    AV_PIX_FMT_RGB48BE,
  /**
   * < packed RGB 16:16:16, 48bpp, 16R, 16G, 16B, the 2-byte value for 
   * each R/G/B component is stored as little-endian
   */
    AV_PIX_FMT_RGB48LE,
  /**
   * < packed RGB 5:6:5, 16bpp, (msb) 5R 6G 5B(lsb), big-endian
   */
    AV_PIX_FMT_RGB565BE,
  /**
   * < packed RGB 5:6:5, 16bpp, (msb) 5R 6G 5B(lsb), little-endian
   */
    AV_PIX_FMT_RGB565LE,
  /**
   * < packed RGB 5:5:5, 16bpp, (msb)1A 5R 5G 5B(lsb), big-endian, most 
   * significant bit to 0
   */
    AV_PIX_FMT_RGB555BE,
  /**
   * < packed RGB 5:5:5, 16bpp, (msb)1A 5R 5G 5B(lsb), little-endian, 
   * most significant bit to 0
   */
    AV_PIX_FMT_RGB555LE,
  /**
   * < packed BGR 5:6:5, 16bpp, (msb) 5B 6G 5R(lsb), big-endian
   */
    AV_PIX_FMT_BGR565BE,
  /**
   * < packed BGR 5:6:5, 16bpp, (msb) 5B 6G 5R(lsb), little-endian
   */
    AV_PIX_FMT_BGR565LE,
  /**
   * < packed BGR 5:5:5, 16bpp, (msb)1A 5B 5G 5R(lsb), big-endian, most 
   * significant bit to 1
   */
    AV_PIX_FMT_BGR555BE,
  /**
   * < packed BGR 5:5:5, 16bpp, (msb)1A 5B 5G 5R(lsb), little-endian, 
   * most significant bit to 1
   */
    AV_PIX_FMT_BGR555LE,
  /**
   * < HW acceleration through VA API at motion compensation entry-point, 
   * Picture.data[3] contains a vaapi_render_state struct which contains 
   * macroblocks as well as various fields extracted from headers
   */
    AV_PIX_FMT_VAAPI_MOCO,
  /**
   * < HW acceleration through VA API at IDCT entry-point, Picture.data[3] 
   * contains a vaapi_render_state struct which contains fields extracted 
   * from headers
   */
    AV_PIX_FMT_VAAPI_IDCT,
  /**
   * < HW decoding through VA API, Picture.data[3] contains a vaapi_render_state 
   * struct which contains the bitstream of the slices as well as various 
   * fields extracted from headers
   */
    AV_PIX_FMT_VAAPI_VLD,
  /**
   * < planar YUV 4:2:0, 24bpp, (1 Cr & Cb sample per 2x2 Y samples), 
   * little-endian
   */
    AV_PIX_FMT_YUV420P16LE,
  /**
   * < planar YUV 4:2:0, 24bpp, (1 Cr & Cb sample per 2x2 Y samples), 
   * big-endian
   */
    AV_PIX_FMT_YUV420P16BE,
  /**
   * < planar YUV 4:2:2, 32bpp, (1 Cr & Cb sample per 2x1 Y samples), 
   * little-endian
   */
    AV_PIX_FMT_YUV422P16LE,
  /**
   * < planar YUV 4:2:2, 32bpp, (1 Cr & Cb sample per 2x1 Y samples), 
   * big-endian
   */
    AV_PIX_FMT_YUV422P16BE,
  /**
   * < planar YUV 4:4:4, 48bpp, (1 Cr & Cb sample per 1x1 Y samples), 
   * little-endian
   */
    AV_PIX_FMT_YUV444P16LE,
  /**
   * < planar YUV 4:4:4, 48bpp, (1 Cr & Cb sample per 1x1 Y samples), 
   * big-endian
   */
    AV_PIX_FMT_YUV444P16BE,
  /**
   * < MPEG4 HW decoding with VDPAU, data[0] contains a vdpau_render_state 
   * struct which contains the bitstream of the slices as well as various 
   * fields extracted from headers
   */
    AV_PIX_FMT_VDPAU_MPEG4,
  /**
   * < HW decoding through DXVA2, Picture.data[3] contains a LPDIRECT3DSURFACE9 
   * pointer
   */
    AV_PIX_FMT_DXVA2_VLD,
  /**
   * < packed RGB 4:4:4, 16bpp, (msb)4A 4R 4G 4B(lsb), little-endian, 
   * most significant bits to 0
   */
    AV_PIX_FMT_RGB444LE,
  /**
   * < packed RGB 4:4:4, 16bpp, (msb)4A 4R 4G 4B(lsb), big-endian, most 
   * significant bits to 0
   */
    AV_PIX_FMT_RGB444BE,
  /**
   * < packed BGR 4:4:4, 16bpp, (msb)4A 4B 4G 4R(lsb), little-endian, 
   * most significant bits to 1
   */
    AV_PIX_FMT_BGR444LE,
  /**
   * < packed BGR 4:4:4, 16bpp, (msb)4A 4B 4G 4R(lsb), big-endian, most 
   * significant bits to 1
   */
    AV_PIX_FMT_BGR444BE,
  /**
   * < 8bit gray, 8bit alpha
   */
    AV_PIX_FMT_GRAY8A,
  /**
   * < packed RGB 16:16:16, 48bpp, 16B, 16G, 16R, the 2-byte value for 
   * each R/G/B component is stored as big-endian
   */
    AV_PIX_FMT_BGR48BE,
  /**
   * < packed RGB 16:16:16, 48bpp, 16B, 16G, 16R, the 2-byte value for 
   * each R/G/B component is stored as little-endian
   */
    AV_PIX_FMT_BGR48LE,
  /**
   * < planar YUV 4:2:0, 13.5bpp, (1 Cr & Cb sample per 2x2 Y samples), 
   * big-endian
   */
    AV_PIX_FMT_YUV420P9BE,
  /**
   * < planar YUV 4:2:0, 13.5bpp, (1 Cr & Cb sample per 2x2 Y samples), 
   * little-endian
   */
    AV_PIX_FMT_YUV420P9LE,
  /**
   * < planar YUV 4:2:0, 15bpp, (1 Cr & Cb sample per 2x2 Y samples), 
   * big-endian
   */
    AV_PIX_FMT_YUV420P10BE,
  /**
   * < planar YUV 4:2:0, 15bpp, (1 Cr & Cb sample per 2x2 Y samples), 
   * little-endian
   */
    AV_PIX_FMT_YUV420P10LE,
  /**
   * < planar YUV 4:2:2, 20bpp, (1 Cr & Cb sample per 2x1 Y samples), 
   * big-endian
   */
    AV_PIX_FMT_YUV422P10BE,
  /**
   * < planar YUV 4:2:2, 20bpp, (1 Cr & Cb sample per 2x1 Y samples), 
   * little-endian
   */
    AV_PIX_FMT_YUV422P10LE,
  /**
   * < planar YUV 4:4:4, 27bpp, (1 Cr & Cb sample per 1x1 Y samples), 
   * big-endian
   */
    AV_PIX_FMT_YUV444P9BE,
  /**
   * < planar YUV 4:4:4, 27bpp, (1 Cr & Cb sample per 1x1 Y samples), 
   * little-endian
   */
    AV_PIX_FMT_YUV444P9LE,
  /**
   * < planar YUV 4:4:4, 30bpp, (1 Cr & Cb sample per 1x1 Y samples), 
   * big-endian
   */
    AV_PIX_FMT_YUV444P10BE,
  /**
   * < planar YUV 4:4:4, 30bpp, (1 Cr & Cb sample per 1x1 Y samples), 
   * little-endian
   */
    AV_PIX_FMT_YUV444P10LE,
  /**
   * < planar YUV 4:2:2, 18bpp, (1 Cr & Cb sample per 2x1 Y samples), 
   * big-endian
   */
    AV_PIX_FMT_YUV422P9BE,
  /**
   * < planar YUV 4:2:2, 18bpp, (1 Cr & Cb sample per 2x1 Y samples), 
   * little-endian
   */
    AV_PIX_FMT_YUV422P9LE,
  /**
   * < hardware decoding through VDA
   */
    AV_PIX_FMT_VDA_VLD,
  /**
   * < planar GBR 4:4:4 24bpp
   */
    AV_PIX_FMT_GBRP,
  /**
   * < planar GBR 4:4:4 27bpp, big-endian
   */
    AV_PIX_FMT_GBRP9BE,
  /**
   * < planar GBR 4:4:4 27bpp, little-endian
   */
    AV_PIX_FMT_GBRP9LE,
  /**
   * < planar GBR 4:4:4 30bpp, big-endian
   */
    AV_PIX_FMT_GBRP10BE,
  /**
   * < planar GBR 4:4:4 30bpp, little-endian
   */
    AV_PIX_FMT_GBRP10LE,
  /**
   * < planar GBR 4:4:4 48bpp, big-endian
   */
    AV_PIX_FMT_GBRP16BE,
  /**
   * < planar GBR 4:4:4 48bpp, little-endian
   */
    AV_PIX_FMT_GBRP16LE,
  /**
   * < planar YUV 4:2:2 24bpp, (1 Cr & Cb sample per 2x1 Y & A samples) 
   *
   */
    AV_PIX_FMT_YUVA422P_LIBAV,
  /**
   * < planar YUV 4:4:4 32bpp, (1 Cr & Cb sample per 1x1 Y & A samples) 
   *
   */
    AV_PIX_FMT_YUVA444P_LIBAV,
  /**
   * < planar YUV 4:2:0 22.5bpp, (1 Cr & Cb sample per 2x2 Y & A samples), 
   * big-endian
   */
    AV_PIX_FMT_YUVA420P9BE,
  /**
   * < planar YUV 4:2:0 22.5bpp, (1 Cr & Cb sample per 2x2 Y & A samples), 
   * little-endian
   */
    AV_PIX_FMT_YUVA420P9LE,
  /**
   * < planar YUV 4:2:2 27bpp, (1 Cr & Cb sample per 2x1 Y & A samples), 
   * big-endian
   */
    AV_PIX_FMT_YUVA422P9BE,
  /**
   * < planar YUV 4:2:2 27bpp, (1 Cr & Cb sample per 2x1 Y & A samples), 
   * little-endian
   */
    AV_PIX_FMT_YUVA422P9LE,
  /**
   * < planar YUV 4:4:4 36bpp, (1 Cr & Cb sample per 1x1 Y & A samples), 
   * big-endian
   */
    AV_PIX_FMT_YUVA444P9BE,
  /**
   * < planar YUV 4:4:4 36bpp, (1 Cr & Cb sample per 1x1 Y & A samples), 
   * little-endian
   */
    AV_PIX_FMT_YUVA444P9LE,
  /**
   * < planar YUV 4:2:0 25bpp, (1 Cr & Cb sample per 2x2 Y & A samples, 
   * big-endian)
   */
    AV_PIX_FMT_YUVA420P10BE,
  /**
   * < planar YUV 4:2:0 25bpp, (1 Cr & Cb sample per 2x2 Y & A samples, 
   * little-endian)
   */
    AV_PIX_FMT_YUVA420P10LE,
  /**
   * < planar YUV 4:2:2 30bpp, (1 Cr & Cb sample per 2x1 Y & A samples, 
   * big-endian)
   */
    AV_PIX_FMT_YUVA422P10BE,
  /**
   * < planar YUV 4:2:2 30bpp, (1 Cr & Cb sample per 2x1 Y & A samples, 
   * little-endian)
   */
    AV_PIX_FMT_YUVA422P10LE,
  /**
   * < planar YUV 4:4:4 40bpp, (1 Cr & Cb sample per 1x1 Y & A samples, 
   * big-endian)
   */
    AV_PIX_FMT_YUVA444P10BE,
  /**
   * < planar YUV 4:4:4 40bpp, (1 Cr & Cb sample per 1x1 Y & A samples, 
   * little-endian)
   */
    AV_PIX_FMT_YUVA444P10LE,
  /**
   * < planar YUV 4:2:0 40bpp, (1 Cr & Cb sample per 2x2 Y & A samples, 
   * big-endian)
   */
    AV_PIX_FMT_YUVA420P16BE,
  /**
   * < planar YUV 4:2:0 40bpp, (1 Cr & Cb sample per 2x2 Y & A samples, 
   * little-endian)
   */
    AV_PIX_FMT_YUVA420P16LE,
  /**
   * < planar YUV 4:2:2 48bpp, (1 Cr & Cb sample per 2x1 Y & A samples, 
   * big-endian)
   */
    AV_PIX_FMT_YUVA422P16BE,
  /**
   * < planar YUV 4:2:2 48bpp, (1 Cr & Cb sample per 2x1 Y & A samples, 
   * little-endian)
   */
    AV_PIX_FMT_YUVA422P16LE,
  /**
   * < planar YUV 4:4:4 64bpp, (1 Cr & Cb sample per 1x1 Y & A samples, 
   * big-endian)
   */
    AV_PIX_FMT_YUVA444P16BE,
  /**
   * < planar YUV 4:4:4 64bpp, (1 Cr & Cb sample per 1x1 Y & A samples, 
   * little-endian)
   */
    AV_PIX_FMT_YUVA444P16LE,
  /**
   * < HW acceleration through VDPAU, Picture.data[3] contains a VdpVideoSurface 
   *
   */
    AV_PIX_FMT_VDPAU,
  /**
   * < packed RGBA 16:16:16:16, 64bpp, 16R, 16G, 16B, 16A, the 2-byte 
   * value for each R/G/B/A component is stored as big-endian
   */
    AV_PIX_FMT_RGBA64BE(VideoJNI.IPixelFormat_AV_PIX_FMT_RGBA64BE_get()),
  /**
   * < packed RGBA 16:16:16:16, 64bpp, 16R, 16G, 16B, 16A, the 2-byte 
   * value for each R/G/B/A component is stored as little-endian
   */
    AV_PIX_FMT_RGBA64LE,
  /**
   * < packed RGBA 16:16:16:16, 64bpp, 16B, 16G, 16R, 16A, the 2-byte 
   * value for each R/G/B/A component is stored as big-endian
   */
    AV_PIX_FMT_BGRA64BE,
  /**
   * < packed RGBA 16:16:16:16, 64bpp, 16B, 16G, 16R, 16A, the 2-byte 
   * value for each R/G/B/A component is stored as little-endian
   */
    AV_PIX_FMT_BGRA64LE,
  /**
   * < packed RGB 8:8:8, 32bpp, 0RGB0RGB...
   */
    AV_PIX_FMT_0RGB(VideoJNI.IPixelFormat_AV_PIX_FMT_0RGB_get()),
  /**
   * < packed RGB 8:8:8, 32bpp, RGB0RGB0...
   */
    AV_PIX_FMT_RGB0,
  /**
   * < packed BGR 8:8:8, 32bpp, 0BGR0BGR...
   */
    AV_PIX_FMT_0BGR,
  /**
   * < packed BGR 8:8:8, 32bpp, BGR0BGR0...
   */
    AV_PIX_FMT_BGR0,
  /**
   * < planar YUV 4:4:4 32bpp, (1 Cr & Cb sample per 1x1 Y & A samples) 
   *
   */
    AV_PIX_FMT_YUVA444P,
  /**
   * < planar YUV 4:2:2 24bpp, (1 Cr & Cb sample per 2x1 Y & A samples) 
   *
   */
    AV_PIX_FMT_YUVA422P,
  /**
   * < planar YUV 4:2:0,18bpp, (1 Cr & Cb sample per 2x2 Y samples), big-endian 
   *
   */
    AV_PIX_FMT_YUV420P12BE,
  /**
   * < planar YUV 4:2:0,18bpp, (1 Cr & Cb sample per 2x2 Y samples), little-endian 
   *
   */
    AV_PIX_FMT_YUV420P12LE,
  /**
   * < planar YUV 4:2:0,21bpp, (1 Cr & Cb sample per 2x2 Y samples), big-endian 
   *
   */
    AV_PIX_FMT_YUV420P14BE,
  /**
   * < planar YUV 4:2:0,21bpp, (1 Cr & Cb sample per 2x2 Y samples), little-endian 
   *
   */
    AV_PIX_FMT_YUV420P14LE,
  /**
   * < planar YUV 4:2:2,24bpp, (1 Cr & Cb sample per 2x1 Y samples), big-endian 
   *
   */
    AV_PIX_FMT_YUV422P12BE,
  /**
   * < planar YUV 4:2:2,24bpp, (1 Cr & Cb sample per 2x1 Y samples), little-endian 
   *
   */
    AV_PIX_FMT_YUV422P12LE,
  /**
   * < planar YUV 4:2:2,28bpp, (1 Cr & Cb sample per 2x1 Y samples), big-endian 
   *
   */
    AV_PIX_FMT_YUV422P14BE,
  /**
   * < planar YUV 4:2:2,28bpp, (1 Cr & Cb sample per 2x1 Y samples), little-endian 
   *
   */
    AV_PIX_FMT_YUV422P14LE,
  /**
   * < planar YUV 4:4:4,36bpp, (1 Cr & Cb sample per 1x1 Y samples), big-endian 
   *
   */
    AV_PIX_FMT_YUV444P12BE,
  /**
   * < planar YUV 4:4:4,36bpp, (1 Cr & Cb sample per 1x1 Y samples), little-endian 
   *
   */
    AV_PIX_FMT_YUV444P12LE,
  /**
   * < planar YUV 4:4:4,42bpp, (1 Cr & Cb sample per 1x1 Y samples), big-endian 
   *
   */
    AV_PIX_FMT_YUV444P14BE,
  /**
   * < planar YUV 4:4:4,42bpp, (1 Cr & Cb sample per 1x1 Y samples), little-endian 
   *
   */
    AV_PIX_FMT_YUV444P14LE,
  /**
   * < planar GBR 4:4:4 36bpp, big-endian
   */
    AV_PIX_FMT_GBRP12BE,
  /**
   * < planar GBR 4:4:4 36bpp, little-endian
   */
    AV_PIX_FMT_GBRP12LE,
  /**
   * < planar GBR 4:4:4 42bpp, big-endian
   */
    AV_PIX_FMT_GBRP14BE,
  /**
   * < planar GBR 4:4:4 42bpp, little-endian
   */
    AV_PIX_FMT_GBRP14LE;

    public final int swigValue() {
      return swigValue;
    }

    public static Type swigToEnum(int swigValue) {
      Type[] swigValues = Type.class.getEnumConstants();
      if (swigValue < swigValues.length && swigValue >= 0 && swigValues[swigValue].swigValue == swigValue)
        return swigValues[swigValue];
      for (Type swigEnum : swigValues)
        if (swigEnum.swigValue == swigValue)
          return swigEnum;
      throw new IllegalArgumentException("No enum " + Type.class + " with value " + swigValue);
    }

    @SuppressWarnings("unused")
    private Type() {
      this.swigValue = SwigNext.next++;
    }

    @SuppressWarnings("unused")
    private Type(int swigValue) {
      this.swigValue = swigValue;
      SwigNext.next = swigValue+1;
    }

    @SuppressWarnings("unused")
    private Type(Type swigEnum) {
      this.swigValue = swigEnum.swigValue;
      SwigNext.next = this.swigValue+1;
    }

    private final int swigValue;

    private static class SwigNext {
      private static int next = 0;
    }
  }

  public enum YUVColorComponent {
    YUV_Y(VideoJNI.IPixelFormat_YUV_Y_get()),
    YUV_U(VideoJNI.IPixelFormat_YUV_U_get()),
    YUV_V(VideoJNI.IPixelFormat_YUV_V_get());

    public final int swigValue() {
      return swigValue;
    }

    public static YUVColorComponent swigToEnum(int swigValue) {
      YUVColorComponent[] swigValues = YUVColorComponent.class.getEnumConstants();
      if (swigValue < swigValues.length && swigValue >= 0 && swigValues[swigValue].swigValue == swigValue)
        return swigValues[swigValue];
      for (YUVColorComponent swigEnum : swigValues)
        if (swigEnum.swigValue == swigValue)
          return swigEnum;
      throw new IllegalArgumentException("No enum " + YUVColorComponent.class + " with value " + swigValue);
    }

    @SuppressWarnings("unused")
    private YUVColorComponent() {
      this.swigValue = SwigNext.next++;
    }

    @SuppressWarnings("unused")
    private YUVColorComponent(int swigValue) {
      this.swigValue = swigValue;
      SwigNext.next = swigValue+1;
    }

    @SuppressWarnings("unused")
    private YUVColorComponent(YUVColorComponent swigEnum) {
      this.swigValue = swigEnum.swigValue;
      SwigNext.next = this.swigValue+1;
    }

    private final int swigValue;

    private static class SwigNext {
      private static int next = 0;
    }
  }

}
